{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette feuille a pour but de travailler a la construction d'un AST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas oublier :\n",
    "lorsque l'AST est construit :\n",
    "    - 1 seul token equals\n",
    "    - 1 partie droit existante\n",
    "    - rajouter les prochains trucs a verifier a posteriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AST(object):\n",
    "    pass\n",
    "\n",
    "class Node_letter(object):\n",
    "    \n",
    "    def __init__(self, token, name):\n",
    "        self.token = token\n",
    "        self.name = name\n",
    "        self.state = 0\n",
    "        self.neg = 0\n",
    "        self.childs_pos = []\n",
    "        self.childs_neg = []\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Node_letter({})\".format(self.token)\n",
    "        \n",
    "class Node_condition(object):\n",
    "    \n",
    "    def __init__(self, left, cond, right):\n",
    "        self.left = left\n",
    "        self.token = self.cond = cond\n",
    "        self.right = right\n",
    "        self.neg = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Node_condition({}\".format(self.token)\n",
    "class Token(object):\n",
    "    def __init__(self, token_type, value):\n",
    "        self.type = token_type\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the class instance.\n",
    "        Examples:\n",
    "            Token(NEG, \"!\")\n",
    "            Token(LETTER, \"A\")\n",
    "            Token(AND, '+')\n",
    "            Token(OR, '|')\n",
    "            Token(XOR, '^')\n",
    "        \"\"\"\n",
    "        return 'Token({type}, {value})'.format(\n",
    "            type=self.type,\n",
    "            value=repr(self.value)\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer(object):\n",
    "    def __init__(self, line):\n",
    "        self.rule = line\n",
    "        \n",
    "        self.pos = 0\n",
    "        self.current_char = self.rule[self.pos]\n",
    "    \n",
    "    def error(self, EOL=False):\n",
    "        if EOL == False:\n",
    "            raise Exception('Invalid character \\'{}\\' at index {}'.format(self.rule[self.pos], self.pos + 1))\n",
    "        else:\n",
    "            raise Exception('Invalid character \\'{}\\' at index {}'.format(\"EOL\", self.pos + 1))\n",
    "    def get_next_token(self):\n",
    "        dic_op = {\n",
    "            \"+\": \"AND\",\n",
    "            \"|\": \"OR\",\n",
    "            \"^\": \"XOR\",\n",
    "            \"!\": \"NOT\",\n",
    "            \"(\": \"OPEN_PAR\",\n",
    "            \")\": \"CLOSE_PAR\"\n",
    "        }\n",
    "        dic_equal = {\n",
    "            \"=\": \"EQUAL\",\n",
    "            \">\": \"IMPLIES\",\n",
    "            \"<\": \"ONLY_IF\"\n",
    "        }\n",
    "        i = 0\n",
    "        len_rule = len(self.rule)\n",
    "        while i <  len_rule:\n",
    "            self.pos = i\n",
    "            if self.rule[i] == \" \":\n",
    "                i += 1\n",
    "                continue\n",
    "            if ord(self.rule[i]) in range(ord(\"A\"), ord(\"Z\") + 1):\n",
    "                yield Token(\"LETTER\", self.rule[i])\n",
    "            elif self.rule[i] in dic_op:\n",
    "                yield Token(dic_op[self.rule[i]], self.rule[i])\n",
    "            elif self.rule[i] in dic_equal:\n",
    "                tmp = self.get_equals_token(i, len_rule)\n",
    "                i += tmp[0]\n",
    "                yield tmp[1]\n",
    "            else:\n",
    "                self.error()\n",
    "            i += 1\n",
    "        yield Token(\"EOL\", None)\n",
    "    \n",
    "    def get_equals_token(self, i, len_rule):\n",
    "        if self.rule[i] == \"<\" and i < len_rule - 2:\n",
    "            if self.rule[i + 1] == \"=\":\n",
    "                if self.rule[i + 2] == \">\":\n",
    "                    return ([2, Token(\"ONLY_IF\", \"<=>\")])\n",
    "                else:\n",
    "                    self.pos += 2\n",
    "                    self.error()\n",
    "            else:\n",
    "                self.pos += 1\n",
    "                self.error()\n",
    "        elif self.rule[i] == \"=\" and i < len_rule - 1:\n",
    "            if self.rule[i + 1] == \">\":\n",
    "                return ([1, Token(\"IMPLIES\", \"=>\")])\n",
    "            else:\n",
    "                self.pos += 1\n",
    "                self.error()\n",
    "        if i == len_rule - 1:\n",
    "            self.pos += 1\n",
    "            self.error(EOL=True)\n",
    "        elif i == len_rule - 2:\n",
    "            self.pos += 2\n",
    "            self.error(EOL=True)\n",
    "        self.error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le principe du Parser repose sur de la recursivitée, en impliquant des priorité sur les tokens rencontrés.\n",
    "\n",
    "Dans notre problème, on peut distinguer 3 niveaux de priorités, du moins prioritaire au plus prioritaire\n",
    "\n",
    "* 1: le token equal. (=> ou <=>)\n",
    "* 2: operateurs : ^, |, +\n",
    "* 3: lettres, parentheses\n",
    "\n",
    "* A noter que la negation n'est pas encore bien integrée a l'arbre\n",
    "\n",
    "\n",
    "la recursivitée est programmée comme ceci :\n",
    "le niveau 1 apelle le niveau 2 qui apelle le niveau 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        self.gen = self.lexer.get_next_token()\n",
    "        self.current_token = next(self.gen)\n",
    "        \n",
    "    def error(self, s):\n",
    "        raise Exception(\"Invalid syntax : {}\".format(s))\n",
    "    \n",
    "    def get_next_token(self, token_type):\n",
    "        if self.current_token.type == token_type:\n",
    "            self.current_token = next(self.gen)\n",
    "        else:\n",
    "            self.error(\"could not find match of parenthesis\")\n",
    "    def parse(self):\n",
    "        \"\"\"\n",
    "        Cette fonction a pour but d'enclencher le debut du parsing recursif.\n",
    "        la variable node va etre le neud representant la racine de l'AST.\n",
    "        \"\"\"\n",
    "        node = self.deep_one()\n",
    "        if self.current_token.type != \"EOL\":\n",
    "            self.error(self.lexer.error())\n",
    "        if (node.token.type != \"IMPLIES\") and (node.token.type != \"ONLY_IF\"):\n",
    "            self.error(\"Missing valid equals sign\")\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    def deep_one(self):\n",
    "        \"\"\"\n",
    "        Cette pronfondeur de prioritée est assignée aux token suivants :\n",
    "            Token(IMPLIES, '=>')\n",
    "            Token(ONLY_IF, '<=>')            \n",
    "            Token(NEG, '!')\n",
    "        \"\"\"\n",
    "        node = self.deep_two()\n",
    "        \n",
    "        \n",
    "        while self.current_token.type in (\"IMPLIES\", \"ONLY_IF\"):\n",
    "            token = self.current_token\n",
    "            if token.type == \"IMPLIES\":\n",
    "                self.get_next_token(\"IMPLIES\")\n",
    "            if token.type == \"ONLY_IF\":\n",
    "                self.get_next_token(\"ONLY_IF\")\n",
    "                \n",
    "            node = Node_condition(left=node, cond=token, right=self.deep_two())\n",
    "            \n",
    "        return node\n",
    "    \n",
    "    def deep_two(self):\n",
    "        \"\"\"\n",
    "        Cette profondeur de prioritée est assignée aux token suivants :\n",
    "            Token(AND, '+')\n",
    "            Token(OR, '|')\n",
    "            Token(XOR, '^')\n",
    "        \"\"\"\n",
    "        node = self.deep_three()\n",
    "        \n",
    "        while self.current_token.type in (\"AND\", \"OR\", \"XOR\"):\n",
    "            token = self.current_token\n",
    "            if token.type == \"AND\":\n",
    "                self.get_next_token(\"AND\")\n",
    "            if token.type == \"OR\":\n",
    "                self.get_next_token(\"OR\")\n",
    "            if token.type == \"XOR\":\n",
    "                self.get_next_token(\"XOR\")\n",
    "            \n",
    "            node = Node_condition(left=node, cond=token, right=self.deep_three())\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    def deep_three(self):\n",
    "        \"\"\"\n",
    "        Cette profondeur de prioritée est assignée au token equals :\n",
    "            Token(LETTER, 'A->Z')\n",
    "            Token(OPEN_PAR, '(' )\n",
    "            Token(CLOSE_PAR, ')' )\n",
    "            Token(NOT, '!')\n",
    "        \"\"\"\n",
    "        token = self.current_token\n",
    "        if token.type == \"LETTER\":\n",
    "            self.get_next_token(\"LETTER\")\n",
    "            return Node_letter(token, token.value)\n",
    "        elif token.type == \"OPEN_PAR\":\n",
    "            self.get_next_token(\"OPEN_PAR\")\n",
    "            node = self.deep_one()\n",
    "            self.get_next_token(\"CLOSE_PAR\")\n",
    "            return node\n",
    "        elif token.type == \"NOT\":\n",
    "            self.get_next_token(\"NOT\")\n",
    "            token = self.current_token\n",
    "            if token.type == \"LETTER\":\n",
    "                self.get_next_token(\"LETTER\")\n",
    "                ret = Node_letter(token, token.value)\n",
    "                ret.neg = 1\n",
    "                return ret\n",
    "            elif token.type == \"OPEN_PAR\":\n",
    "                self.get_next_token(\"OPEN_PAR\")\n",
    "                node = self.deep_one()\n",
    "                node.neg = 1\n",
    "                self.get_next_token(\"CLOSE_PAR\")\n",
    "                return node\n",
    "        else:\n",
    "            self.lexer.error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule_interpreter(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "#         self.root = root\n",
    "#         self.set_facts(root, true_facts)\n",
    "        \n",
    "    def interpret(self, node):\n",
    "        if node:\n",
    "            if type(node).__name__ == \"Node_condition\":\n",
    "                return abs(self.apply_logical(node, node.left, node.right) - node.neg)\n",
    "            if type(node).__name__ == \"Node_letter\":\n",
    "                for child in (node.childs_pos + node.childs_neg):\n",
    "                    if self.interpret(child) == 1:\n",
    "                        node.state = 1\n",
    "#                 for child in node.childs_neg: # ZIP HERE\n",
    "#                     if self.interpret(child) == 1:\n",
    "#                         node.state\n",
    "                return abs(node.state - node.neg)\n",
    "    \n",
    "    def apply_logical(self, node, left, right):\n",
    "        if node.token.type == \"AND\":\n",
    "            return self.interpret(left) & self.interpret(right)\n",
    "        if node.token.type == \"OR\":\n",
    "            return self.interpret(left) | self.interpret(right)\n",
    "        if node.token.type == \"XOR\":\n",
    "            return self.interpret(left) ^ self.interpret(right)\n",
    "    \n",
    "    def set_facts(self, node, true_facts):\n",
    "        if node:\n",
    "            if type(node).__name__ == \"Node_letter\":\n",
    "                if node.token.value in true_facts:\n",
    "                    node.state = 1\n",
    "            if type(node).__name__ == \"Node_condition\":\n",
    "                self.set_facts(node.left, true_facts)\n",
    "                self.set_facts(node.right, true_facts)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "rule1 = \"A ^ B => D\"\n",
    "rule2 = \"A + B => D\"\n",
    "true_facts = \"AB\"\n",
    "try:\n",
    "    lexer = Lexer(rule1)\n",
    "    parser = Parser(lexer)\n",
    "    root = parser.parse()\n",
    "    interpretor = Rule_interpreter(root.left, true_facts)\n",
    "    print(interpretor.interpret(root.left))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "    \n",
    "    def __init__(self, true_facts):\n",
    "        self.implies_list = []\n",
    "        self.implies_name_list = []\n",
    "        self.true_facts = true_facts\n",
    "        self.interpretor = Rule_interpreter()\n",
    "        \n",
    "    def add_new_AST(self, root):\n",
    "        self.update_implies_list(root.right)\n",
    "        self.update_graph(root.left, root.right)\n",
    "        \n",
    "    \n",
    "    def update_implies_list(self, node):\n",
    "        if node:\n",
    "            if type(node).__name__ == \"Node_letter\":\n",
    "                if node.name not in self.implies_name_list:\n",
    "                    self.implies_list.append(node)\n",
    "                    self.implies_name_list.append(node.name)\n",
    "            if type(node).__name__ == \"Node_condition\":\n",
    "                self.update_implies_list(node.left)\n",
    "                self.update_implies_list(node.right)\n",
    "    \n",
    "    def update_graph(self, left, node):\n",
    "        if node:\n",
    "            if type(node).__name__ == \"Node_letter\":\n",
    "                if node.neg == 1:\n",
    "                    self.find_letter_in_implies(node.name).childs_neg.append(left)\n",
    "                else:\n",
    "                    self.find_letter_in_implies(node.name).childs_pos.append(left)\n",
    "            if type(node).__name__ == \"Node_condition\":\n",
    "                self.update_graph(left, node.left)\n",
    "                self.update_graph(left, node.right)\n",
    "    \n",
    "        \n",
    "    def find_letter_in_implies(self, name):\n",
    "        for node in self.implies_list:\n",
    "            if node.name == name:\n",
    "                return node\n",
    "        raise Exception(\"Error node not found\")\n",
    "    \n",
    "\n",
    "    def set_facts(self, node):\n",
    "        if node:\n",
    "            if type(node).__name__ == \"Node_letter\":\n",
    "                for child in (node.childs_pos + node.childs_neg):\n",
    "                    self.set_facts(child)\n",
    "                if node.token.value in self.true_facts:\n",
    "                    node.state = 1\n",
    "            if type(node).__name__ == \"Node_condition\":\n",
    "                self.set_facts(node.left)\n",
    "                self.set_facts(node.right)\n",
    "                \n",
    "    \n",
    "    def query(self, letter):\n",
    "        if letter in self.full_history.keys():\n",
    "            print(\"{} is {}\".format(letter, self.full_history[letter]))\n",
    "            return self.full_history[letter]\n",
    "        elif letter in self.true_facts:\n",
    "            print(\"{} is True\".format(letter))\n",
    "            return True\n",
    "        else:\n",
    "            print(\"{} is False\".format(letter))\n",
    "            return False\n",
    "    \n",
    "    def check_contradiction(self):\n",
    "        self.full_history = {}\n",
    "        for node in self.implies_list:\n",
    "            self.full_history[node.name] = self.get_final_state(node)\n",
    "            if self.full_history[node.name] == None:\n",
    "                if node.name in self.true_facts:\n",
    "                    self.full_history[node.name] = True\n",
    "                else:\n",
    "                    self.full_history[node.name] = False\n",
    "                    \n",
    "    def get_final_state(self, node):\n",
    "        history = []\n",
    "        for child in node.childs_pos:\n",
    "            res = self.interpretor.interpret(child)\n",
    "            if res == 1:\n",
    "                history.append(True)\n",
    "                \n",
    "        for child in node.childs_neg:\n",
    "            res = self.interpretor.interpret(child)\n",
    "            if res == 1:\n",
    "                history.append(False)\n",
    "            \n",
    "        if len(set(history)) == 0:\n",
    "            return\n",
    "        elif len(set(history)) == 1:\n",
    "            return history[0]\n",
    "        else:\n",
    "            raise Exception(\"Error contradiction found with letter {}\".format(node.name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is False\n",
      "B is True\n",
      "F is False\n",
      "D is False\n"
     ]
    }
   ],
   "source": [
    "rule1 = \"A + B => D\"\n",
    "# rule1bis = \"B + C => !D\"\n",
    "# rule1ter = \"(B + C) | A => D\"\n",
    "rule2 = \"E + C => B + !A\"\n",
    "rule3 = \"G => H\"\n",
    "\n",
    "# rule1 = \"A + B => !C\"\n",
    "# rule2 = \"D + E => F\"\n",
    "# rule3 = \"G + H => I\"\n",
    "# rule2 = \"C => A\"\n",
    "\n",
    "\n",
    "true_facts = \"AEC\"\n",
    "\n",
    "try:\n",
    "    graph = Graph(true_facts)\n",
    "    \n",
    "    root1 = Parser(Lexer(rule1)).parse()\n",
    "    graph.add_new_AST(root1)\n",
    "    \n",
    "    root2 = Parser(Lexer(rule2)).parse()\n",
    "    graph.add_new_AST(root2)\n",
    "    \n",
    "    root3 = Parser(Lexer(rule3)).parse()\n",
    "    graph.add_new_AST(root3)\n",
    "    \n",
    "    for node in graph.implies_list:\n",
    "        graph.set_facts(node)\n",
    "    \n",
    "    graph.check_contradiction()\n",
    "    \n",
    "    graph.query(\"A\")\n",
    "    graph.query(\"B\")\n",
    "    graph.query(\"F\")\n",
    "    graph.query(\"D\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_letter(Token(LETTER, 'D')) D 0\n",
      "Node_letter(Token(LETTER, 'B')) B 0\n",
      "Node_letter(Token(LETTER, 'A')) A 1\n",
      "Node_letter(Token(LETTER, 'H')) H 0\n"
     ]
    }
   ],
   "source": [
    "for c in graph.implies_list:\n",
    "    print(c, c.name, c.neg)\n",
    "# print(graph.implies_list[0].childs[0].left)\n",
    "# print(graph.check_contrast())\n",
    "# print()\n",
    "# for c in graph.implies_list[0].childs:\n",
    "#     print(c.left, c.left.childs[0].opp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_letter(Token(LETTER, 'D')) 0\n",
      "Node_condition(Token(AND, '+') 0\n",
      "Node_letter(Token(LETTER, 'A')) 0\n",
      "Node_condition(Token(AND, '+') 1\n",
      "Node_letter(Token(LETTER, 'E')) 0\n",
      "Node_letter(Token(LETTER, 'C')) 0\n",
      "Node_letter(Token(LETTER, 'B')) 0\n",
      "Node_condition(Token(AND, '+') 1\n",
      "Node_letter(Token(LETTER, 'E')) 0\n",
      "Node_letter(Token(LETTER, 'C')) 0\n",
      "Node_condition(Token(AND, '+') 0\n",
      "Node_letter(Token(LETTER, 'A')) 0\n",
      "Node_condition(Token(AND, '+') 1\n",
      "Node_letter(Token(LETTER, 'E')) 0\n",
      "Node_letter(Token(LETTER, 'C')) 0\n",
      "Node_letter(Token(LETTER, 'B')) 0\n",
      "Node_condition(Token(AND, '+') 1\n",
      "Node_letter(Token(LETTER, 'E')) 0\n",
      "Node_letter(Token(LETTER, 'C')) 0\n"
     ]
    }
   ],
   "source": [
    "prefix_run(graph.implies_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_run(node):\n",
    "    if node:\n",
    "        print(node, node.opp)\n",
    "        if type(node).__name__ == \"Node_letter\":\n",
    "            for c in node.childs:\n",
    "                prefix_run(c)\n",
    "        if type(node).__name__ == \"Node_condition\":\n",
    "            prefix_run(node.left)\n",
    "            prefix_run(node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "a[\"A\"] = 12\n",
    "if \"B\" in a.keys():\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rule1c = \"A ^ B => !D\"\n",
    "# rule2 = \"(A+B) | (A ^ B) => D\"\n",
    "\n",
    "rule3 = \"A + B => D\"\n",
    "rule4 = \"A ^ B => D\"\n",
    "rules5 = \"A => !B\"\n",
    "ruleA = \"!A => B\"\n",
    "ruleB = \"A => B\"\n",
    "try:\n",
    "    root1 = Parser(Lexer(rule1)).parse()\n",
    "    root2 = Parser(Lexer(rule2)).parse()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
