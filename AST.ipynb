{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette feuille a pour but de travailler a la construction d'un AST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas oublier :\n",
    "lorsque l'AST est construit :\n",
    "    - 1 seul token equals\n",
    "    - 1 partie droit existante\n",
    "    - rajouter les prochains trucs a verifier a posteriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AST(object):\n",
    "    pass\n",
    "\n",
    "class Node_letter(object):\n",
    "    \n",
    "    def __init__(self, token, name):\n",
    "        self.token = token\n",
    "        self.name = name\n",
    "        self.state = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Node_letter({})\".format(self.token)\n",
    "        \n",
    "class Node_condition(object):\n",
    "    \n",
    "    def __init__(self, left, cond, right):\n",
    "        self.left = left\n",
    "        self.token = self.cond = cond\n",
    "        self.right = right\n",
    "        #self.not = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Node_condition({}\".format(self.token)\n",
    "class Token(object):\n",
    "    def __init__(self, token_type, value):\n",
    "        self.type = token_type\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the class instance.\n",
    "        Examples:\n",
    "            Token(NEG, \"!\")\n",
    "            Token(LETTER, \"A\")\n",
    "            Token(AND, '+')\n",
    "            Token(OR, '|')\n",
    "            Token(XOR, '^')\n",
    "        \"\"\"\n",
    "        return 'Token({type}, {value})'.format(\n",
    "            type=self.type,\n",
    "            value=repr(self.value)\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer(object):\n",
    "    def __init__(self, line):\n",
    "        self.rule = line\n",
    "        \n",
    "        self.pos = 0\n",
    "        self.current_char = self.rule[self.pos]\n",
    "    \n",
    "    def error(self, EOL=False):\n",
    "        if EOL == False:\n",
    "            raise Exception('Invalid character \\'{}\\' at index {}'.format(self.rule[self.pos], self.pos + 1))\n",
    "        else:\n",
    "            raise Exception('Invalid character \\'{}\\' at index {}'.format(\"EOL\", self.pos + 1))\n",
    "    def get_next_token(self):\n",
    "        dic_op = {\n",
    "            \"+\": \"AND\",\n",
    "            \"|\": \"OR\",\n",
    "            \"^\": \"XOR\",\n",
    "            \"!\": \"NOT\",\n",
    "            \"(\": \"OPEN_PAR\",\n",
    "            \")\": \"CLOSE_PAR\"\n",
    "        }\n",
    "        dic_equal = {\n",
    "            \"=\": \"EQUAL\",\n",
    "            \">\": \"IMPLIES\",\n",
    "            \"<\": \"ONLY_IF\"\n",
    "        }\n",
    "        i = 0\n",
    "        len_rule = len(self.rule)\n",
    "        while i <  len_rule:\n",
    "            self.pos = i\n",
    "            if self.rule[i] == \" \":\n",
    "                i += 1\n",
    "                continue\n",
    "            if ord(self.rule[i]) in range(ord(\"A\"), ord(\"Z\") + 1):\n",
    "                yield Token(\"LETTER\", self.rule[i])\n",
    "            elif self.rule[i] in dic_op:\n",
    "                yield Token(dic_op[self.rule[i]], self.rule[i])\n",
    "            elif self.rule[i] in dic_equal:\n",
    "                tmp = self.get_equals_token(i, len_rule)\n",
    "                i += tmp[0]\n",
    "                yield tmp[1]\n",
    "            else:\n",
    "                self.error()\n",
    "            i += 1\n",
    "        yield Token(\"EOL\", None)\n",
    "    \n",
    "    def get_equals_token(self, i, len_rule):\n",
    "        if self.rule[i] == \"<\" and i < len_rule - 2:\n",
    "            if self.rule[i + 1] == \"=\":\n",
    "                if self.rule[i + 2] == \">\":\n",
    "                    return ([2, Token(\"ONLY_IF\", \"<=>\")])\n",
    "                else:\n",
    "                    self.pos += 2\n",
    "                    self.error()\n",
    "            else:\n",
    "                self.pos += 1\n",
    "                self.error(self.rule[i + 1])\n",
    "        elif self.rule[i] == \"=\" and i < len_rule - 1:\n",
    "            if self.rule[i + 1] == \">\":\n",
    "                return ([1, Token(\"IMPLIES\", \"=>\")])\n",
    "            else:\n",
    "                self.pos += 1\n",
    "                self.error()\n",
    "        if i == len_rule - 1:\n",
    "            self.pos += 1\n",
    "            self.error(EOL=True)\n",
    "        elif i == len_rule - 2:\n",
    "            self.pos += 2\n",
    "            self.error(EOL=True)\n",
    "        self.error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le principe du Parser repose sur de la recursivitée, en impliquant des priorité sur les tokens rencontrés.\n",
    "\n",
    "Dans notre problème, on peut distinguer 3 niveaux de priorités, du moins prioritaire au plus prioritaire\n",
    "\n",
    "* 1: le token equal. (=> ou <=>)\n",
    "* 2: operateurs : ^, |, +\n",
    "* 3: lettres, parentheses\n",
    "\n",
    "* A noter que la negation n'est pas encore bien integrée a l'arbre\n",
    "\n",
    "\n",
    "la recursivitée est programmée comme ceci :\n",
    "le niveau 1 apelle le niveau 2 qui apelle le niveau 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        self.gen = self.lexer.get_next_token()\n",
    "        self.current_token = next(self.gen)\n",
    "        \n",
    "    def error(self, s):\n",
    "        raise Exception(\"Invalid syntax : {}\".format(s))\n",
    "    \n",
    "    def get_next_token(self, token_type):\n",
    "        if self.current_token.type == token_type:\n",
    "            self.current_token = next(self.gen)\n",
    "        else:\n",
    "            self.error(\"could not find match of parenthesis\")\n",
    "    def parse(self):\n",
    "        \"\"\"\n",
    "        Cette fonction a pour but d'enclencher le debut du parsing recursif.\n",
    "        la variable node va etre le neud representant la racine de l'AST.\n",
    "        \"\"\"\n",
    "        node = self.deep_one()\n",
    "        if self.current_token.type != \"EOL\":\n",
    "            self.error(self.lexer.error())\n",
    "        if (node.token.type != \"IMPLIES\") and (node.token.type != \"ONLY_IF\"):\n",
    "            self.error(\"Missing valid equals sign\")\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    def deep_one(self):\n",
    "        \"\"\"\n",
    "        Cette pronfondeur de prioritée est assignée aux token suivants :\n",
    "            Token(IMPLIES, '=>')\n",
    "            Token(ONLY_IF, '<=>')            \n",
    "            Token(NEG, '!')\n",
    "        \"\"\"\n",
    "        node = self.deep_two()\n",
    "        \n",
    "        \n",
    "        while self.current_token.type in (\"IMPLIES\", \"ONLY_IF\", \"NEG\"):\n",
    "            token = self.current_token\n",
    "            if token.type == \"IMPLIES\":\n",
    "                self.get_next_token(\"IMPLIES\")\n",
    "            if token.type == \"ONLY_IF\":\n",
    "                self.get_next_token(\"ONLY_IF\")\n",
    "            if token.type == \"NEG\":\n",
    "                ### TO DO\n",
    "                self.get_next_token(\"NEG\")\n",
    "                \n",
    "            node = Node_condition(left=node, cond=token, right=self.deep_two())\n",
    "            \n",
    "        return node\n",
    "    \n",
    "    def deep_two(self):\n",
    "        \"\"\"\n",
    "        Cette profondeur de prioritée est assignée aux token suivants :\n",
    "            Token(AND, '+')\n",
    "            Token(OR, '|')\n",
    "            Token(XOR, '^')\n",
    "        \"\"\"\n",
    "        node = self.deep_three()\n",
    "        \n",
    "        while self.current_token.type in (\"AND\", \"OR\", \"XOR\"):\n",
    "            token = self.current_token\n",
    "            if token.type == \"AND\":\n",
    "                self.get_next_token(\"AND\")\n",
    "            if token.type == \"OR\":\n",
    "                self.get_next_token(\"OR\")\n",
    "            if token.type == \"XOR\":\n",
    "                self.get_next_token(\"XOR\")\n",
    "            \n",
    "            node = Node_condition(left=node, cond=token, right=self.deep_three())\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    def deep_three(self):\n",
    "        \"\"\"\n",
    "        Cette profondeur de prioritée est assignée au token equals :\n",
    "            Token(LETTER, 'A->Z')\n",
    "            Token(OPEN_PAR, '(' )\n",
    "            Token(CLOSE_PAR, ')' )\n",
    "        \"\"\"\n",
    "        token = self.current_token\n",
    "        if token.type == \"LETTER\":\n",
    "            self.get_next_token(\"LETTER\")\n",
    "            return Node_letter(token, token.value)\n",
    "        elif token.type == \"OPEN_PAR\":\n",
    "            self.get_next_token(\"OPEN_PAR\")\n",
    "            node = self.deep_one()\n",
    "            self.get_next_token(\"CLOSE_PAR\")\n",
    "            return node\n",
    "        else:\n",
    "            self.lexer.error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule_interpreter(object):\n",
    "    \n",
    "    def __init__(self, root, true_facts):\n",
    "        self.root = root\n",
    "        self.set_facts(root, true_facts)\n",
    "        \n",
    "    def interpret(self, node):\n",
    "        if node:\n",
    "            if type(node).__name__ == \"Node_condition\":\n",
    "                return self.apply_logical(node, node.left, node.right)\n",
    "            if type(node).__name__ == \"Node_letter\":\n",
    "                return node.state\n",
    "    \n",
    "    def apply_logical(self, node, left, right):\n",
    "        if node.token.type == \"AND\":\n",
    "            return self.interpret(left) & self.interpret(right)\n",
    "        if node.token.type == \"OR\":\n",
    "            return self.interpret(left) | self.interpret(right)\n",
    "        if node.token.type == \"XOR\":\n",
    "            return self.interpret(left) ^ self.interpret(right)\n",
    "    \n",
    "    def set_facts(self, node, true_facts):\n",
    "        if node:\n",
    "            if type(node).__name__ == \"Node_letter\":\n",
    "                if node.token.value in true_facts:\n",
    "                    node.state = 1\n",
    "            if type(node).__name__ == \"Node_condition\":\n",
    "                self.set_facts(node.left, true_facts)\n",
    "                self.set_facts(node.right, true_facts)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "rule = \"A+((B|C)^(E|F)) => (D)\"\n",
    "true_facts = \"ABC\"\n",
    "try:\n",
    "    lexer = Lexer(rule)\n",
    "    parser = Parser(lexer)\n",
    "    root = parser.parse()\n",
    "    interpretor = Rule_interpreter(root.left, true_facts)\n",
    "    print(interpretor.interpret(root.left))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_run(root):\n",
    "    if root:\n",
    "        print(root)\n",
    "        if type(root).__name__ == \"Node_condition\":\n",
    "            prefix_run(root.left)\n",
    "            prefix_run(root.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node_condition(Token(AND, '+')\n",
      "Node_letter(Token(LETTER, 'A'))\n",
      "Node_condition(Token(OR, '|')\n",
      "Node_letter(Token(LETTER, 'B'))\n",
      "Node_letter(Token(LETTER, 'C'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prefix_run(root.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpretor.interpret(root.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"A\" in \"B\":\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
