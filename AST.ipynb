{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette feuille a pour but de travailler a la construction d'un AST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas oublier :\n",
    "lorsque l'AST est construit :\n",
    "    - 1 seul token equals\n",
    "    - 1 partie droit existante\n",
    "    - rajouter les prochains trucs a verifier a posteriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(AND, '+')\n",
      "Token(OR, '|')\n",
      "Token(XOR, '^')\n",
      "Token(EOL, None)\n"
     ]
    }
   ],
   "source": [
    "class AST(object):\n",
    "    pass\n",
    "\n",
    "class Node_letter(object):\n",
    "    \n",
    "    def __init__(self, token, name):\n",
    "        self.token = token\n",
    "        self.name = name\n",
    "        self.state = 0\n",
    "        \n",
    "class Node_condition(object):\n",
    "    \n",
    "    def __init__(self, left, cond, right):\n",
    "        self.left = left\n",
    "        self.token = self.cond = cond\n",
    "        self.right = right\n",
    "        #self.not = 0\n",
    "        \n",
    "class Token(object):\n",
    "    def __init__(self, token_type, value):\n",
    "        self.type = token_type\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the class instance.\n",
    "        Examples:\n",
    "            Token(NEG, \"!\")\n",
    "            Token(LETTER, \"A\")\n",
    "            Token(AND, '+')\n",
    "            Token(OR, '|')\n",
    "            Token(XOR, '^')\n",
    "        \"\"\"\n",
    "        return 'Token({type}, {value})'.format(\n",
    "            type=self.type,\n",
    "            value=repr(self.value)\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(AND, '+')\n",
      "Token(OR, '|')\n",
      "Token(XOR, '^')\n",
      "Token(EOL, None)\n"
     ]
    }
   ],
   "source": [
    "class Lexer(object):\n",
    "    def __init__(self, line):\n",
    "        self.rule = line\n",
    "        \n",
    "        self.pos = 0\n",
    "        self.current_char = self.rule[self.pos]\n",
    "        for e in self.get_next_token():\n",
    "            print(e)\n",
    "    \n",
    "    def error(self, char):\n",
    "        raise Exception('Invalid character \\'{}\\' at index {}'.format(char, self.pos + 1))\n",
    "\n",
    "    def get_next_token(self):\n",
    "        dic_op = {\n",
    "            \"+\": \"AND\",\n",
    "            \"|\": \"OR\",\n",
    "            \"^\": \"XOR\",\n",
    "            \"!\": \"NOT\",\n",
    "            \"(\": \"OPEN_PAR\",\n",
    "            \")\": \"CLOSE_PAR\"\n",
    "        }\n",
    "        dic_equal = {\n",
    "            \"=\": \"EQUAL\",\n",
    "            \">\": \"IMPLIES\",\n",
    "            \"<\": \"ONLY_IF\"\n",
    "        }\n",
    "        i = 0\n",
    "        len_rule = len(self.rule)\n",
    "        while i <  len_rule:\n",
    "            self.pos = i\n",
    "            if self.rule[i] == \" \":\n",
    "                i += 1\n",
    "                continue\n",
    "            if ord(self.rule[i]) in range(ord(\"A\"), ord(\"Z\") + 1):\n",
    "                yield Token(\"LETTER\", self.rule[i])\n",
    "            elif self.rule[i] in dic_op:\n",
    "                yield Token(dic_op[self.rule[i]], self.rule[i])\n",
    "            elif self.rule[i] in dic_equal:\n",
    "                tmp = self.get_equals_token(i, len_rule)\n",
    "                i += tmp[0]\n",
    "                yield tmp[1]\n",
    "            else:\n",
    "                self.error(self.rule[i])\n",
    "            i += 1\n",
    "        yield Token(\"EOL\", None)\n",
    "    \n",
    "    def get_equals_token(self, i, len_rule):\n",
    "        if self.rule[i] == \"<\" and i < len_rule - 2:\n",
    "            if self.rule[i + 1] == \"=\":\n",
    "                if self.rule[i + 2] == \">\":\n",
    "                    return ([2, Token(\"ONLY_IF\", \"<=>\")])\n",
    "                else:\n",
    "                    self.pos += 2\n",
    "                    self.error(self.rule[i + 2])\n",
    "            else:\n",
    "                self.pos += 1\n",
    "                self.error(self.rule[i + 1])\n",
    "        elif self.rule[i] == \"=\" and i < len_rule - 1:\n",
    "            if self.rule[i + 1] == \">\":\n",
    "                return ([1, Token(\"IMPLIES\", \"=>\")])\n",
    "            else:\n",
    "                self.pos += 1\n",
    "                self.error(self.rule[i + 1])\n",
    "        if i == len_rule - 1:\n",
    "            self.pos += 1\n",
    "            self.error(\"EOL\")\n",
    "        elif i == len_rule - 2:\n",
    "            self.pos += 2\n",
    "            self.error(\"EOL\")\n",
    "        self.error(self.rule[i])\n",
    "        \n",
    "lexer = Lexer(\"+|^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le principe du Parser repose sur de la recursivitée, en impliquant des priorité sur les tokens rencontrés.\n",
    "\n",
    "Dans notre problème, on peut distinguer 3 niveaux de priorités, du moins prioritaire au plus prioritaire\n",
    "\n",
    "* 1: operateurs : ^, |, +\n",
    "* 2: lettres, parentheses, !\n",
    "* 3: le token equal. (=> ou <=>)\n",
    "\n",
    "la recursivitée est programmée comme ceci :\n",
    "le niveau 1 apelle le niveau 2 qui apelle le niveau 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        self.current_token = self.lexer.get_next_token()\n",
    "        \n",
    "    def error(self, s):\n",
    "        raise Exception(\"Invalid syntax : {}\".format(s))\n",
    "        \n",
    "    def parse(self):\n",
    "        \"\"\"\n",
    "        Cette fonction a pour but d'enclencher le debut du parsing recursif.\n",
    "        la variable node va etre le neud representant la racine de l'AST.\n",
    "        \"\"\"\n",
    "        node = self.deep_one()\n",
    "        if self.current_token.type != \"EOL\":\n",
    "            self.error(\"parsing ended without EOL\")\n",
    "        return node\n",
    "    \n",
    "    def deep_one(self):\n",
    "        return\n",
    "    \n",
    "    def deep_two(self):\n",
    "        return\n",
    "    \n",
    "    def deep_three(self):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
